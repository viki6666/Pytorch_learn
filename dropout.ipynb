{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dropout.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viki6666/Pytorch_learn/blob/master/dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrL0rC1hQ2MV",
        "colab_type": "text"
      },
      "source": [
        "Dropout理解"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lALsvVmnGuG4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "ef0c0786-b3c7-4e9c-ed9d-d4679911fd21"
      },
      "source": [
        "# coding:utf-8\n",
        "import numpy as np\n",
        " \n",
        "# dropout函数的实现\n",
        "def dropout(x, level):\n",
        "    if level < 0. or level >= 1: #level是概率值，必须在0~1之间\n",
        "        raise ValueError('Dropout level must be in interval [0, 1[.')\n",
        "    retain_prob = 1. - level\n",
        " \n",
        "    # 我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样\n",
        "    # 硬币 正面的概率为p，n表示每个神经元试验的次数\n",
        "    # 因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。\n",
        "    random_tensor = np.random.binomial(n=1, p=retain_prob, size=x.shape) #即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了\n",
        "    print(random_tensor)\n",
        " \n",
        "    x *= random_tensor\n",
        "    print(x)\n",
        "    x /= retain_prob\n",
        " \n",
        "    return x\n",
        " \n",
        "#对dropout的测试，大家可以跑一下上面的函数，了解一个输入x向量，经过dropout的结果  \n",
        "x=np.asarray([1,2,3,4,5,6,7,8,9,10],dtype=np.float32)\n",
        "dropout(x,0.4)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 0 1 0 0 1 0 1]\n",
            "[ 0.  0.  3.  0.  5.  0.  0.  8.  0. 10.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.      ,  0.      ,  5.      ,  0.      ,  8.333333,  0.      ,\n",
              "        0.      , 13.333333,  0.      , 16.666666], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3X2KUWnRAkl",
        "colab_type": "text"
      },
      "source": [
        "Dropout实现"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU39LwfnVIMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 前向传播，需要用到一个输入x以及所有的权重以及偏执项，都在parameters这个字典里面存储\n",
        "# 最后返回会返回一个caches里面包含的 是各层的a和z，a[layers]就是最终的输出\n",
        "def forward(x,parameters,keep_prob = 0.5):\n",
        "    a = []\n",
        "    z = []\n",
        "    d = []\n",
        "    caches = {}\n",
        "    a.append(x)\n",
        "    z.append(x)\n",
        "    # 输入层不用删除\n",
        "    d.append(np.ones(x.shape))\n",
        "    layers = len(parameters)//2\n",
        "    # 前面都要用sigmoid\n",
        "    for i in range(1,layers):\n",
        "        z_temp =parameters[\"w\"+str(i)].dot(a[i-1]) + parameters[\"b\"+str(i)]#参数和数据进行计算\n",
        "        a_temp = sigmoid(z_temp)#激活函数\n",
        "        # 1、建立一个维度与本层神经元数目相同的矩阵d_temp.\n",
        "        d_temp = np.random.rand(z_temp.shape[0],z_temp.shape[1])\n",
        "        #2、根据概率(keep_prob)我们把d_temp中的元素设置为0或者1.\n",
        "        d_temp = d_temp < keep_prob\n",
        "        #3、把本层的激活函数的输出a_temp 与 d_temp相乘（对应元素乘）作为新的输出。4、除keep_prob\n",
        "        a_temp = (a_temp * d_temp)/keep_prob\n",
        "        z.append(z_temp)\n",
        "        a.append(a_temp)\n",
        "        d.append(d_temp)\n",
        "        \n",
        "    # 最后一层不用sigmoid,也不用dropout\n",
        "    z_temp = parameters[\"w\"+str(layers)].dot(a[layers-1]) + parameters[\"b\"+str(layers)]\n",
        "    z.append(z_temp)\n",
        "    a.append(z_temp)\n",
        "    d.append(np.ones(z_temp.shape))\n",
        "    \n",
        "    caches[\"z\"] = z\n",
        "    caches[\"a\"] = a\n",
        "    # 记得保存起来，因为反向传播还会使用\n",
        "    caches[\"d\"] = d\n",
        "    caches[\"keep_prob\"] = keep_prob    \n",
        "    return  caches,a[layers]   \n",
        "    \n",
        "    \n",
        "# 反向传播，parameters里面存储的是所有的各层的权重以及偏执，caches里面存储各层的a和z\n",
        "# al是经过反向传播后最后一层的输出，y代表真实值 \n",
        "# 返回的grades代表着误差对所有的w以及b的导数\n",
        "def backward(parameters,caches,al,y):\n",
        "    layers = len(parameters)//2#整除\n",
        "    grades = {}\n",
        "    m = y.shape[1]\n",
        "    # 假设最后一层不经历激活函数\n",
        "    # 就是按照上面的图片中的公式写的\n",
        "    grades[\"dz\"+str(layers)] = al - y\n",
        "    grades[\"dw\"+str(layers)] = grades[\"dz\"+str(layers)].dot(caches[\"a\"][layers-1].T) /m\n",
        "    grades[\"db\"+str(layers)] = np.sum(grades[\"dz\"+str(layers)],axis = 1,keepdims = True) /m\n",
        "    # 前面全部都是sigmoid激活\n",
        "    for i in reversed(range(1,layers)):\n",
        "        da_temp = parameters[\"w\"+str(i+1)].T.dot(grades[\"dz\"+str(i+1)])\n",
        "        #5、在进行反向传播的时候同样进行这样的操作，所以要把D存储下来，这样才能对被关闭的神经元对应的w不进行更新\n",
        "        # 要记得乘上对应的开关caches[\"d\"][i]，这样就保证了关闭的神经元在反向传播的时候仍然是关闭的\n",
        "        da_temp = (caches[\"d\"][i] * da_temp)/caches[\"keep_prob\"]\n",
        "        grades[\"dz\"+str(i)] = da_temp * sigmoid_prime(caches[\"z\"][i])\n",
        "        grades[\"dw\"+str(i)] = grades[\"dz\"+str(i)].dot(caches[\"a\"][i-1].T)/m\n",
        "        grades[\"db\"+str(i)] = np.sum(grades[\"dz\"+str(i)],axis = 1,keepdims = True) /m\n",
        "    return grades   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8LWCrxGP1TX",
        "colab_type": "text"
      },
      "source": [
        "![替代文字](https://img-blog.csdnimg.cn/20181120152355651.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4ODg4ODM3,size_16,color_FFFFFF,t_70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1v2V6yedqu-",
        "colab_type": "text"
      },
      "source": [
        "BP神经网络  \n",
        "[代码来源](https://blog.csdn.net/qq_28888837/article/details/84296673)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fshp0Z__YO4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 生成权重以及偏执项layers_dim代表每层的神经元个数，\n",
        "#比如[2,3,1]代表一个三成的网络，输入为2层，中间为3层输出为1层\n",
        "def init_parameters(layers_dim):\n",
        "    \n",
        "    L = len(layers_dim)\n",
        "    parameters ={}\n",
        "    for i in range(1,L):\n",
        "        parameters[\"w\"+str(i)] = np.random.random([layers_dim[i],layers_dim[i-1]])\n",
        "        parameters[\"b\"+str(i)] = np.zeros((layers_dim[i],1))\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c74307Xgdetn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "# sigmoid的导函数\n",
        "def sigmoid_prime(z):\n",
        "        return sigmoid(z) * (1-sigmoid(z))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymx2XNDDdicN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 前向传播，需要用到一个输入x以及所有的权重以及偏执项，都在parameters这个字典里面存储\n",
        "# 最后返回会返回一个caches里面包含的 是各层的a和z，a[layers]就是最终的输出\n",
        "def forward(x,parameters):\n",
        "    a = []\n",
        "    z = []\n",
        "    caches = {}\n",
        "    a.append(x)\n",
        "    z.append(x)\n",
        "    layers = len(parameters)//2\n",
        "    # 前面都要用sigmoid\n",
        "    for i in range(1,layers):\n",
        "        z_temp =parameters[\"w\"+str(i)].dot(x) + parameters[\"b\"+str(i)]\n",
        "        z.append(z_temp)\n",
        "        a.append(sigmoid(z_temp))\n",
        "    # 最后一层不用sigmoid\n",
        "    z_temp = parameters[\"w\"+str(layers)].dot(a[layers-1]) + parameters[\"b\"+str(layers)]\n",
        "    z.append(z_temp)\n",
        "    a.append(z_temp)\n",
        "    \n",
        "    caches[\"z\"] = z\n",
        "    caches[\"a\"] = a    \n",
        "    return  caches,a[layers]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwR2n46QdnC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 反向传播，parameters里面存储的是所有的各层的权重以及偏执，caches里面存储各层的a和z\n",
        "# al是经过反向传播后最后一层的输出，y代表真实值\n",
        "# 返回的grades代表着误差对所有的w以及b的导数\n",
        "def backward(parameters,caches,al,y):\n",
        "    layers = len(parameters)//2\n",
        "    grades = {}\n",
        "    m = y.shape[1]\n",
        "    # 假设最后一层不经历激活函数\n",
        "    # 就是按照上面的图片中的公式写的\n",
        "    grades[\"dz\"+str(layers)] = al - y\n",
        "    grades[\"dw\"+str(layers)] = grades[\"dz\"+str(layers)].dot(caches[\"a\"][layers-1].T) /m\n",
        "    grades[\"db\"+str(layers)] = np.sum(grades[\"dz\"+str(layers)],axis = 1,keepdims = True) /m\n",
        "    # 前面全部都是sigmoid激活\n",
        "    for i in reversed(range(1,layers)):\n",
        "        grades[\"dz\"+str(i)] = parameters[\"w\"+str(i+1)].T.dot(grades[\"dz\"+str(i+1)]) * sigmoid_prime(caches[\"z\"][i])\n",
        "        grades[\"dw\"+str(i)] = grades[\"dz\"+str(i)].dot(caches[\"a\"][i-1].T)/m\n",
        "        grades[\"db\"+str(i)] = np.sum(grades[\"dz\"+str(i)],axis = 1,keepdims = True) /m\n",
        "    return grades   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMzrlA1id2yX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 就是把其所有的权重以及偏执都更新一下\n",
        "def update_grades(parameters,grades,learning_rate):\n",
        "    layers = len(parameters)//2\n",
        "    for i in range(1,layers+1):\n",
        "        parameters[\"w\"+str(i)] -= learning_rate * grades[\"dw\"+str(i)]\n",
        "        parameters[\"b\"+str(i)] -= learning_rate * grades[\"db\"+str(i)]\n",
        "    return parameters\n",
        "# 计算误差值\n",
        "def compute_loss(al,y):\n",
        "    return np.mean(np.square(al-y))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wa3OWMHd599",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# 加载数据\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    加载数据集\n",
        "    \"\"\"\n",
        "    x = np.arange(0.0,1.0,0.01)#0-1 0.01\n",
        "    y =20* np.sin(2*np.pi*x)\n",
        "    # 数据可视化\n",
        "    plt.scatter(x,y)\n",
        "    return x,y\n",
        "#进行测试\n",
        "x,y = load_data()\n",
        "x = x.reshape(1,100)\n",
        "y = y.reshape(1,100)\n",
        "plt.scatter(x,y)\n",
        "parameters = init_parameters([1,25,1])\n",
        "al = 0\n",
        "for i in range(4000):\n",
        "    caches,al = forward(x, parameters)\n",
        "    grades = backward(parameters, caches, al, y)\n",
        "    parameters = update_grades(parameters, grades, learning_rate= 0.3)\n",
        "    if i %100 ==0:\n",
        "        print(compute_loss(al, y))\n",
        "plt.scatter(x,al)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7yCs3_2d78l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "last layer without sigmoid\n",
        "@author: cdq\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "def dsigmoid(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "class BP(object):\n",
        "    def __init__(self, layers, activation='sigmoid', learning_rate=0.01):\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.caches = {}\n",
        "        self.grades = {}\n",
        "        if activation == 'sigmoid':\n",
        "            self.activation = sigmoid\n",
        "            self.dactivation = dsigmoid\n",
        "        self.parameters = {}\n",
        "        for i in range(1, len(self.layers)):\n",
        "            self.parameters[\"w\"+str(i)] = np.random.random((self.layers[i], self.layers[i-1]))\n",
        "            self.parameters[\"b\"+str(i)] = np.zeros((layers[i],1))\n",
        "    \n",
        "    def forward(self, X):\n",
        "        a = []\n",
        "        z = []\n",
        "        a.append(X)\n",
        "        z.append(X)\n",
        "        \n",
        "        len_layers = len(self.parameters) // 2\n",
        "        for i in range(1, len_layers):\n",
        "            z.append(self.parameters[\"w\"+str(i)] @ a[i-1] + self.parameters[\"b\"+str(i)])\n",
        "            a.append(sigmoid(z[-1]))\n",
        "        #last layer without sigmoid\n",
        "        z.append(self.parameters[\"w\"+str(len_layers)] @ a[-1] + self.parameters[\"b\"+str(len_layers)])\n",
        "        a.append(z[-1])\n",
        "        \n",
        "        self.caches['z'] = z\n",
        "        self.caches['a'] = a\n",
        "        \n",
        "        return self.caches, a[-1]\n",
        "    \n",
        "    def backward(self, y):\n",
        "        a = self.caches['a']\n",
        "        m = y.shape[1]\n",
        "        # last layer grade\n",
        "        len_layers = len(self.parameters) // 2\n",
        "        self.grades[\"dz\"+str(len_layers)] = a[-1]-y\n",
        "        self.grades[\"dw\"+str(len_layers)] = self.grades[\"dz\"+str(len_layers)].dot(a[-2].T) / m\n",
        "        self.grades[\"db\"+str(len_layers)] = np.sum(self.grades[\"dz\"+str(len_layers)], axis=1, keepdims=True) / m\n",
        "        # compute grades\n",
        "        for i in reversed(range(1, len_layers)):\n",
        "            self.grades[\"dz\"+str(i)] = self.parameters[\"w\"+str(i+1)].T.dot(self.grades[\"dz\"+str(i+1)]) * dsigmoid(self.caches[\"z\"][i])\n",
        "            self.grades[\"dw\"+str(i)] = self.grades[\"dz\"+str(i)].dot(self.caches[\"a\"][i-1].T)/m\n",
        "            self.grades[\"db\"+str(i)] = np.sum(self.grades[\"dz\"+str(i)],axis = 1,keepdims = True) /m\n",
        "        #update weights and bias\n",
        "        for i in range(1, len(self.layers)):\n",
        "            self.parameters[\"w\"+str(i)] -= self.learning_rate * self.grades[\"dw\"+str(i)]\n",
        "            self.parameters[\"b\"+str(i)] -= self.learning_rate * self.grades[\"db\"+str(i)]\n",
        "            \n",
        "    def compute_loss(self, y):\n",
        "        return np.mean(np.square(self.caches['a'][-1]-y))\n",
        "#%%\n",
        "def test():\n",
        "    x = np.arange(0.0,1.0,0.01)\n",
        "    y =20* np.sin(2*np.pi*x)\n",
        "    plt.scatter(x,y)\n",
        "    \n",
        "    x = x.reshape(1, 100)\n",
        "    y = y.reshape(1, 100)\n",
        "    \n",
        "    bp = BP([1, 6, 1], learning_rate = 0.01)\n",
        "    \n",
        "    for i in range(1, 50000):\n",
        "        caches, al = bp.forward(x)\n",
        "        bp.backward(y)\n",
        "        \n",
        "        if(i%50 == 0):\n",
        "            print(bp.compute_loss(y))\n",
        "    plt.scatter(x, al)\n",
        "    plt.show()\n",
        "    \n",
        "test()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}